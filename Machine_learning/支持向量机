一般来讲，SVM肯定绕不开KKT
svm来源于最优间隔分类器，而最优间隔分类器的求解过程是个有不等式约束的解
高等数学学过等式约束的求最优解是用拉格朗日乘子法(原理就是梯度共线)
KKT就是扩展了拉格朗日乘子法(目标函数的格式没咋变，还叫拉格朗日函数)，在求解有不等式约束的优化问题时使用
形如 L(a, b, x)= f(x) + a*g(x)+b*h(x)  s.t.(g(x)<=0.h(x)=0)
KKT条件是说，如果w是最优解，那w一定满足KKT条件，
也就是说KKT条件是w是最优解的必要条件，当原函数是凸函数时满足KKT条件的解一定是最优解，即充分必要(就像最优解一定导数为0，导数为0不一定是最优解，当且仅当凸函数)
唯一需要注意的条件就是a*g(x) = 0，为什么说唯一需要注意呢，因为其他条件都是诸如L求偏导要等于0之类的废话
因为这个条件的存在，使得很多ai都等于0，剩下的不等于0的ai所对应的xi就叫支持向量，至于怎么解ai后面再说
求导等于0之后会得到一个重要的结论，f(x)=g(wTx + b)中的w是<xi,x>(内积）的线性组合，因为内积的存在使我们能引入核函数减少计算
