一般来讲，SVM肯定绕不开KKT
svm来源于最优间隔分类器，而最优间隔分类器的求解过程是个有不等式约束的解
高等数学学过等式约束的求最优解是用拉格朗日乘子法(原理就是梯度共线)
KKT就是扩展了拉格朗日乘子法(目标函数的格式没咋变，还叫拉格朗日函数)，在求解有不等式约束的优化问题时使用
形如 L(a, b, x)= f(x) + a*g(x)+b*h(x)  s.t.(g(x)<=0.h(x)=0)
KKT条件是说，如果w是最优解，那w一定满足KKT条件，
也就是说KKT条件是w是最优解的必要条件，当原函数是凸函数时满足KKT条件的解一定是最优解，即充分必要(就像最优解一定导数为0，导数为0不一定是最优解，当且仅当凸函数)
唯一需要注意的条件就是a*g(x) = 0，为什么说唯一需要注意呢，因为其他条件都是诸如L求偏导要等于0之类的废话
因为这个条件的存在，使得很多ai都等于0，剩下的不等于0的ai所对应的xi就叫支持向量，至于怎么解ai后面再讲（SMO）
对原问题的对偶问题求导等于0之后会得到一个重要的结论，f(x)=g(wTx + b)中的w是<xi,x>(内积）的线性组合，因为内积的存在使我们能引入核函数减少计算
对于任何问题，如果能引入内积，都可以通过核进行计算，比如logistic。吴恩达是通过对偶问题引入内积的，李宏毅是通过梯度求导引入的，感觉更简单。
SMO算法解ai，
坐标上升法是固定其他的a，只改变某一个ai，求w最优时的ai，再改变其他的a继续迭代
但SVM的约束条件使得我们不能单独的只改变某一个ai，所以我们改变某一对ai，aj，哈哈
