对于线性回归，我们通过拟合θ来找到 ∑i(y(i)−θTx(i))2 的最小值
预测的值为 θTx

而对于局部加权回归，我们通过拟合θ来找到 ∑iw(i)(y(i)−θTx(i))2 的最小值
预测的值仍为 θTx

这里的 w(i)是权重，它并非一个定值，我们通过调节w(i)的值来确定不同训练数据对结果的影响力，
w(i)=exp(−(x(i)−x)22τ2) 是比较常用的一种W（i），很像高斯函数，但和高斯函数并没有什么关系。

刚开始接触到Loess时，觉得很‘正确’，因为更看重局部信息；但仔细一想，loess的θ该是多少维的呢，对每一个x(注意这里的x不是样本x(i)),θ都会不一样，
无限细分x岂不是会有无限个θ了。

其实不是，我们可以换一个思维，从线性回归的思维中跳出来，线性回归是求出拟合全部曲线的θ，对于要预测的数据，均采用同一组θ预测，
但我们不需要拟合全部曲线，给我们一个要预测的X数据，我们只需要求出拟合x周围的x(i)的θ，再求θTx就行了，所以每预测一个x，都要重新计算θ。

其实这就是 参数学习方法 和 非参数学习方法的区别了。
首先参数学习方法是这样一种方法：在训练完成所有数据后得到一系列训练参数，然后根据训练参数来预测新样本的值，这时不再依赖之前的训练数据了，参数值是确定的。
而非参数学习方法是这样一种算法：在预测新样本值时候每次都会重新训练数据得到新的参数值，也就是说每次预测新样本都会依赖训练数据集合，所以每次得到的
参数值是不确定的。
